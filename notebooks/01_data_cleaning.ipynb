{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e032114e",
   "metadata": {},
   "source": [
    "## Notebook 1 : 01_data_cleaning.ipynb\n",
    "\n",
    "#### Author: Satveer Kaur\n",
    "#### Date: 2025-10-18\n",
    "#### Notebook Purpose:\n",
    "This notebook performs **initial data ingestion, cleaning, and structural preparation** on the full loan portfolio dataset.\n",
    "\n",
    "#### Primary Goal:\n",
    "To establish a stable, high-quality base DataFrame that is ready for feature engineering in subsequent notebooks.\n",
    "\n",
    "#### Key Actions:\n",
    "1.  **Ingestion:** Load the full, raw loan data to ensure total portfolio volume is preserved.\n",
    "2.  **Data Hygiene:** Identify and manage missing values in critical columns (e.g., FICO, DTI) to prevent calculation errors.\n",
    "3.  **Structural Cleaning:** Standardize data types, convert date fields, and remove irrelevant identifier or highly sparse columns.\n",
    "\n",
    "#### Context:\n",
    "You are a data analyst at LendingClub, and accurate data preparation is critical for building reliable credit risk analytics models and dashboards. The clean output of this notebook serves as the foundation for quantifying loan performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9955825c",
   "metadata": {},
   "source": [
    "#### 1. Setup and Load Raw Data\n",
    "**Purpose**: Initialize all core libraries (`pandas`, `numpy`) required for data manipulation and cleaning. This section loads the full, raw loan data and performs initial checks to prepare the data for quality assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "387940ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load full_loan_data CSV\n",
    "df = pd.read_csv('../data/raw/full_loan_data.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43fd1c3",
   "metadata": {},
   "source": [
    "#### 2. Initial Data Quality Assessment\n",
    "**Purpose:** Quickly assess the data's structure, identify the scale of the missing data problems, and locate columns that may be immediate candidates for removal due to excessive nulls or irrelevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "572b1fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 2,260,668 | Total columns: 102\n",
      "\n",
      "Top 20 Columns with Missing Data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_3c560\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3c560_level0_col0\" class=\"col_heading level0 col0\" >Missing Count</th>\n",
       "      <th id=\"T_3c560_level0_col1\" class=\"col_heading level0 col1\" >Missing Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3c560_level0_row0\" class=\"row_heading level0 row0\" >mths_since_recent_revol_delinq</th>\n",
       "      <td id=\"T_3c560_row0_col0\" class=\"data row0 col0\" >1,520,309</td>\n",
       "      <td id=\"T_3c560_row0_col1\" class=\"data row0 col1\" >67.25%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c560_level0_row1\" class=\"row_heading level0 row1\" >next_pymnt_d</th>\n",
       "      <td id=\"T_3c560_row1_col0\" class=\"data row1 col0\" >1,345,310</td>\n",
       "      <td id=\"T_3c560_row1_col1\" class=\"data row1 col1\" >59.51%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c560_level0_row2\" class=\"row_heading level0 row2\" >mths_since_last_delinq</th>\n",
       "      <td id=\"T_3c560_row2_col0\" class=\"data row2 col0\" >1,158,502</td>\n",
       "      <td id=\"T_3c560_row2_col1\" class=\"data row2 col1\" >51.25%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c560_level0_row3\" class=\"row_heading level0 row3\" >il_util</th>\n",
       "      <td id=\"T_3c560_row3_col0\" class=\"data row3 col0\" >1,068,850</td>\n",
       "      <td id=\"T_3c560_row3_col1\" class=\"data row3 col1\" >47.28%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c560_level0_row4\" class=\"row_heading level0 row4\" >mths_since_rcnt_il</th>\n",
       "      <td id=\"T_3c560_row4_col0\" class=\"data row4 col0\" >909,924</td>\n",
       "      <td id=\"T_3c560_row4_col1\" class=\"data row4 col1\" >40.25%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c560_level0_row5\" class=\"row_heading level0 row5\" >all_util</th>\n",
       "      <td id=\"T_3c560_row5_col0\" class=\"data row5 col0\" >866,348</td>\n",
       "      <td id=\"T_3c560_row5_col1\" class=\"data row5 col1\" >38.32%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c560_level0_row6\" class=\"row_heading level0 row6\" >open_acc_6m</th>\n",
       "      <td id=\"T_3c560_row6_col0\" class=\"data row6 col0\" >866,130</td>\n",
       "      <td id=\"T_3c560_row6_col1\" class=\"data row6 col1\" >38.31%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c560_level0_row7\" class=\"row_heading level0 row7\" >total_cu_tl</th>\n",
       "      <td id=\"T_3c560_row7_col0\" class=\"data row7 col0\" >866,130</td>\n",
       "      <td id=\"T_3c560_row7_col1\" class=\"data row7 col1\" >38.31%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c560_level0_row8\" class=\"row_heading level0 row8\" >inq_last_12m</th>\n",
       "      <td id=\"T_3c560_row8_col0\" class=\"data row8 col0\" >866,130</td>\n",
       "      <td id=\"T_3c560_row8_col1\" class=\"data row8 col1\" >38.31%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c560_level0_row9\" class=\"row_heading level0 row9\" >open_rv_24m</th>\n",
       "      <td id=\"T_3c560_row9_col0\" class=\"data row9 col0\" >866,129</td>\n",
       "      <td id=\"T_3c560_row9_col1\" class=\"data row9 col1\" >38.31%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c560_level0_row10\" class=\"row_heading level0 row10\" >open_il_12m</th>\n",
       "      <td id=\"T_3c560_row10_col0\" class=\"data row10 col0\" >866,129</td>\n",
       "      <td id=\"T_3c560_row10_col1\" class=\"data row10 col1\" >38.31%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c560_level0_row11\" class=\"row_heading level0 row11\" >open_rv_12m</th>\n",
       "      <td id=\"T_3c560_row11_col0\" class=\"data row11 col0\" >866,129</td>\n",
       "      <td id=\"T_3c560_row11_col1\" class=\"data row11 col1\" >38.31%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c560_level0_row12\" class=\"row_heading level0 row12\" >total_bal_il</th>\n",
       "      <td id=\"T_3c560_row12_col0\" class=\"data row12 col0\" >866,129</td>\n",
       "      <td id=\"T_3c560_row12_col1\" class=\"data row12 col1\" >38.31%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c560_level0_row13\" class=\"row_heading level0 row13\" >open_il_24m</th>\n",
       "      <td id=\"T_3c560_row13_col0\" class=\"data row13 col0\" >866,129</td>\n",
       "      <td id=\"T_3c560_row13_col1\" class=\"data row13 col1\" >38.31%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c560_level0_row14\" class=\"row_heading level0 row14\" >open_act_il</th>\n",
       "      <td id=\"T_3c560_row14_col0\" class=\"data row14 col0\" >866,129</td>\n",
       "      <td id=\"T_3c560_row14_col1\" class=\"data row14 col1\" >38.31%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c560_level0_row15\" class=\"row_heading level0 row15\" >max_bal_bc</th>\n",
       "      <td id=\"T_3c560_row15_col0\" class=\"data row15 col0\" >866,129</td>\n",
       "      <td id=\"T_3c560_row15_col1\" class=\"data row15 col1\" >38.31%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c560_level0_row16\" class=\"row_heading level0 row16\" >inq_fi</th>\n",
       "      <td id=\"T_3c560_row16_col0\" class=\"data row16 col0\" >866,129</td>\n",
       "      <td id=\"T_3c560_row16_col1\" class=\"data row16 col1\" >38.31%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c560_level0_row17\" class=\"row_heading level0 row17\" >mths_since_recent_inq</th>\n",
       "      <td id=\"T_3c560_row17_col0\" class=\"data row17 col0\" >295,435</td>\n",
       "      <td id=\"T_3c560_row17_col1\" class=\"data row17 col1\" >13.07%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c560_level0_row18\" class=\"row_heading level0 row18\" >emp_title</th>\n",
       "      <td id=\"T_3c560_row18_col0\" class=\"data row18 col0\" >166,969</td>\n",
       "      <td id=\"T_3c560_row18_col1\" class=\"data row18 col1\" >7.39%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c560_level0_row19\" class=\"row_heading level0 row19\" >num_tl_120dpd_2m</th>\n",
       "      <td id=\"T_3c560_row19_col0\" class=\"data row19 col0\" >153,657</td>\n",
       "      <td id=\"T_3c560_row19_col1\" class=\"data row19 col1\" >6.80%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1e2695323f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data Type Summary (Top) ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2260668 entries, 0 to 2260698\n",
      "Columns: 102 entries, id to total_il_high_credit_limit\n",
      "dtypes: datetime64[ns](1), float64(84), object(17)\n",
      "memory usage: 3.5 GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# check data shape and columns\n",
    "print(f'Total rows: {df.shape[0]:,.0f} | Total columns: {df.shape[1]}')\n",
    "\n",
    "# Analyze Missing Values (Top 20 Columns)\n",
    "# Calculate the percentage of missing values for all columns\n",
    "missing_data = df.isna().sum().sort_values(ascending=False)\n",
    "missing_percentage = (missing_data / len(df)) * 100\n",
    "\n",
    "# Combine into a DataFrame and display the top 20\n",
    "missing_info = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Missing Percent': missing_percentage\n",
    "})\n",
    "\n",
    "print('\\nTop 20 Columns with Missing Data')\n",
    "# Filter to show only columns with at least one missing value\n",
    "display(missing_info[missing_info['Missing Count'] > 0].head(20).style.format({'Missing Count': '{:,.0f}', 'Missing Percent': '{:.2f}%'}))\n",
    "\n",
    "# 3. Quick Data Type Review (for cleaning planning)\n",
    "print(\"\\n--- Data Type Summary (Top) ---\")\n",
    "print(df.info(verbose=False, memory_usage='deep'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da80bc6f",
   "metadata": {},
   "source": [
    "#### 3. Structural Cleaning and Irrelevant Column Removal \n",
    "**Purpose:** Streamline the DataFrame by dropping non-analytical, redundant, or excessively null columns that would introduce noise into the feature engineering process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0e594c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped non-analytical and high-null columns.\n",
      "Columns remaining: 102 (Dropped 19 columns)\n"
     ]
    }
   ],
   "source": [
    "# Drop Redundant and Non-Analytical Columns\n",
    "# These columns are either unique identifiers, text descriptions, or related to post-default debt-settlement actions which are irrelevant for initial risk assessment.\n",
    "drop_cols = [\n",
    "    'member_id', 'url', 'desc', 'title', 'pymnt_plan', 'initial_list_status',\n",
    "    'out_prncp_inv', # post-issue metric\n",
    "    \n",
    "    # Debt Settlement and Hardship flags (post-default actions)\n",
    "    'hardship_flag', 'hardship_type', 'hardship_reason', 'hardship_status',\n",
    "    'deferral_term', 'hardship_amount', 'hardship_start_date', 'hardship_end_date',\n",
    "    'payment_plan_start_date', 'hardship_length', 'hardship_dpd',\n",
    "    'hardship_loan_status', 'orig_projected_additional_accrued_interest',\n",
    "    'hardship_payoff_balance_amount', 'hardship_last_payment_amount',\n",
    "    'disbursement_method', 'debt_settlement_flag', 'debt_settlement_flag_date',\n",
    "    'settlement_status', 'settlement_date', 'settlement_amount',\n",
    "    'settlement_percentage', 'settlement_term'\n",
    "]\n",
    "\n",
    "df.drop(columns=drop_cols, inplace=True,errors='ignore')  # Avoid errors if some columns name don't exist\n",
    "\n",
    "# Drop Columns based on high null threshold\n",
    "# Removing columns that are > 70% empty\n",
    "threshold = 0.7\n",
    "cols_before = df.shape[1]\n",
    "df = df.loc[:, df.isnull().mean() < threshold]\n",
    "cols_after = df.shape[1]\n",
    "\n",
    "print(f'Dropped non-analytical and high-null columns.')\n",
    "print(f'Columns remaining: {cols_after} (Dropped {cols_before - cols_after} columns)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7984e913",
   "metadata": {},
   "source": [
    "#### 4. Column Renaming and Standardization\n",
    "**Purpose:** Rename the remaining columns to use clear, standardized names that match analytical conventions and improve readability for the Tableau dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d042a7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns renamed for clarity.\n",
      "\n",
      "New Columns:\n",
      "['id', 'amount_requested', 'funded_amount', 'funded_amount_invested', 'term', 'interest_rate', 'installment', 'grade', 'sub_grade', 'emp_title']\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary for renaming\n",
    "rename_dict = {\n",
    "    'loan_amnt': 'amount_requested',\n",
    "    'funded_amnt': 'funded_amount',\n",
    "    'funded_amnt_inv': 'funded_amount_invested',\n",
    "    'int_rate': 'interest_rate',\n",
    "    'emp_length': 'employment_length',\n",
    "    'annual_inc': 'annual_income',\n",
    "    'dti': 'debt_to_income_ratio',\n",
    "    'addr_state': 'state',\n",
    "    'fico_range_low': 'fico_low',          # Simplified name\n",
    "    'fico_range_high': 'fico_high',        # Simplified name\n",
    "    'delinq_2yrs': 'delinquencies_2yrs',\n",
    "    'open_acc': 'open_accounts',\n",
    "    'pub_rec': 'public_records',\n",
    "    'revol_bal': 'revolving_balance',\n",
    "    'revol_util': 'revolving_utilization',\n",
    "    'total_acc': 'total_accounts',\n",
    "    'issue_d': 'issue_date',\n",
    "    'loan_status': 'loan_status'\n",
    "}\n",
    "\n",
    "df.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "# Preview first 10 columns to confirm\n",
    "print('Columns renamed for clarity.')\n",
    "print('\\nNew Columns:')\n",
    "print(df.columns[:10].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6202c5",
   "metadata": {},
   "source": [
    "#### 5. Data Type Finalization and Conversion\n",
    "**Purpose:** Convert key columns to their correct numerical and datetime formats to enable calculations and time-series analysis. This is critical for preventing errors in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d659e0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types converted to required format (datetime and numeric)\n",
      "\n",
      "--- Final Data Types for Key Fields ---\n",
      "issue_date       datetime64[ns]\n",
      "annual_income           float64\n",
      "interest_rate           float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Explicitly define columns that should be float/int\n",
    "numeric_cols = [\n",
    "    'amount_requested', 'funded_amount', 'funded_amount_invested', 'interest_rate',\n",
    "    'installment', 'annual_income', 'debt_to_income_ratio', 'fico_low', 'fico_high',\n",
    "    'delinquencies_2yrs', 'open_accounts', 'public_records', 'revolving_balance',\n",
    "    'revolving_utilization', 'total_accounts', 'policy_code'\n",
    "]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce') # invalid parsing wil be set to NaN\n",
    "\n",
    "# Convert dates to datetime\n",
    "df['issue_date'] = pd.to_datetime(df['issue_date'], format= '%b-%Y', errors='coerce') # invalid parsing will be set as NaT\n",
    "\n",
    "print('Data types converted to required format (datetime and numeric)')\n",
    "print('\\n--- Final Data Types for Key Fields ---')\n",
    "print(df[['issue_date', 'annual_income', 'interest_rate']].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55ad7b6",
   "metadata": {},
   "source": [
    "#### 6. Managing Remaining Missing Values (Imputation)\n",
    "**Purpose:** Finalize the data cleaning by handling the remaining null values in critical or useful columns using appropriate imputation strategies to maximize row retention for the Tableau dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb61a594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing 146,940 nulls in employment_length.\n",
      "Remaining critical null values imputed or dropped.\n",
      "Total rows dropped due to critical nulls: 33\n",
      "Total rows remaining after final clean-up: 2,260,668\n"
     ]
    }
   ],
   "source": [
    "# Fill NaN with 'UNKNOWN' to ensure all loans are kept and \"missingness\" becomes its own analytical category.\n",
    "print(f\"Imputing {df['employment_length'].isnull().sum():,.0f} nulls in employment_length.\")\n",
    "df['employment_length'] = df['employment_length'].fillna('UNKNOWN')\n",
    "\n",
    "# Use the median to fill missing values, as it is less sensitive to outliers than the mean.\n",
    "revol_util_median = df['revolving_utilization'].median()\n",
    "df['revolving_utilization'] = df['revolving_utilization'].fillna(revol_util_median)\n",
    "\n",
    "# Drop any rows where critical fields (loan_status, issue_date) are still null. \n",
    "# These rows are fundamentally useless for the analysis.\n",
    "rows_before_drop = len(df)\n",
    "df.dropna(subset=['issue_date', 'loan_status'], inplace=True)\n",
    "rows_after_drop = len(df)\n",
    "\n",
    "print('Remaining critical null values imputed or dropped.')\n",
    "print(f'Total rows dropped due to critical nulls: {rows_before_drop - rows_after_drop:,}')\n",
    "print(f'Total rows remaining after final clean-up: {len(df):,.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84507474",
   "metadata": {},
   "source": [
    "#### 7. Checkpoint and Export\n",
    "**Purpose:** Save the cleaned, stabilized DataFrame to the processed folder. This file will be the input for the next notebook in the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "db07b33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook 1 Complete. Clean data Saved.\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned dataframe for use in the next notebook\n",
    "df.to_csv('../data/processed/clean_data_for_sampling.csv', index=False)\n",
    "\n",
    "print(f'Notebook 1 Complete. Clean data Saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc408e28",
   "metadata": {},
   "source": [
    "##### 5.3 Summary and Next Steps\n",
    "##### Summary\n",
    "\n",
    "1. **Project Stabilization:** Successfully initiated the project by loading the full loan portfolio and stabilizing the data structure.\n",
    "\n",
    "2. **Structural Cleaning:** Dropped highly sparse, redundant, and non-analytical columns reducing the DataFrame size significantly.\n",
    "\n",
    "3. **Data Hygiene:** Managed all remaining missing values in critical columns. Nulls in categorical fields (like `employment_length`) were imputed with UNKNOWN to preserve loan volume, while numerical nulls (`revolving_utilization`) were imputed with the median.\n",
    "\n",
    "4. **Output Integrity:** The final cleaned DataFrame is now structurally sound and contains 2,260,668 records, making it the reliable base for all subsequent exploration.\n",
    "\n",
    "##### Next Steps: Defining Analytical Path\n",
    "\n",
    "The data is now clean, but the core risk drivers must be identified and quantified. The next phase will be exploratory, focusing on isolating the best features for segmentation and risk analysis.\n",
    "\n",
    "**Action:** Proceed to Notebook 02 to begin iterative investigation.\n",
    "\n",
    "1. **Target Variable Definition:** The initial step will involve defining the binary target variable, `is_default`, by consolidating terminal `loan_status` categories (e.g., 'Charged Off', 'Default') to quantify the rate of non-performance.\n",
    "\n",
    "2. **Analytical Efficiency:** To accelerate feature development and testing against the 2.26 million record dataset, a statistically representative sample will be created to optimize the iterative analysis loop.\n",
    "\n",
    "3. **Feature Validation and Selection:** Investigation will commence on potential risk drivers (e.g., FICO Score, Debt-to-Income Ratio (DTI), and Annual Income) to identify features exhibiting a clear, monotonic relationship with the Observed Default Rate. These findings will determine the final segmentation features for reporting.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
