{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79bd0915",
   "metadata": {},
   "source": [
    "# Credit Risk & Loan Performance: Data Sampling and Exploratory Data Analysis\n",
    "\n",
    "#### Author: Satveer Kaur\n",
    "#### Date: 2025-10-19\n",
    "#### Notebook Purpose:\n",
    "This notebook focuses on **data sampling and initial exploratory data analysis (EDA)** for the LendingClub Accepted and Rejected Loans datasets. LendingClub Accepted Loans dataset. \n",
    "The goal is to:\n",
    "1. Create **representative sample datasets** for faster, efficient exploration while preserving the distribution of key variables.\n",
    "2. Conduct **initial EDA**, including univariate analysis of important numerical and categorical features, to understand data structure and detect potential issues.\n",
    "3. Evaluate **variable distributions, proportions, and data balance** between accepted and rejected loans\n",
    "4. Prepare the groundwork for deeper **statistical analysis, feature engineering, and visualization** in subsequent notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c264e48c",
   "metadata": {},
   "source": [
    "#### 1. Load Cleaned Datasets\n",
    "**Purpose**: Import the **cleaned Accepted Loans** CSV files and verify successful loading by checking their shape and basic structure. This ensures dataset is ready for sampling and further exploratory analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70a71ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepted Loans: (2260701, 102)\n"
     ]
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split  # for stratified sampling\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "# To see all the columns in the df\n",
    "# pd.set_option('display.max_columns', False)\n",
    "\n",
    "\n",
    "# Load cleaned datasets\n",
    "accepted_loans = pd.read_csv(\"../data/clean_data/accepted_loans_cleaned.csv\", low_memory=False)\n",
    "\n",
    "print(f\"Accepted Loans: {accepted_loans.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da46455c",
   "metadata": {},
   "source": [
    "#### 2. Create Sample Datasets\n",
    "**Purpose:**  \n",
    "The cleaned datasets are large, which can make visualization and analysis slower. To enable efficient exploratory data analysis (EDA), we create **representative samples** that retain the overall data distribution while reducing size.  \n",
    "\n",
    "This approach allows for faster testing, plotting, and insight generation â€” especially useful when working on limited local resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c7ff0b",
   "metadata": {},
   "source": [
    "##### 2.1 Drop Rows with NaN in `loan_status` ,`annual_income` and `fico_range_high` and ensure correct Data Types.\n",
    "**Purpose:**  \n",
    "For predictive risk modeling, the dataset must be complete for the most critical variables. Rows missing the Target Variable (`loan_status_grouped`) or the Core Predictors (`annual_income`, `fico_range_high`) are unusable, as these fields are essential for measuring and pricing risk.\n",
    "\n",
    "We drop these incomplete rows to maintain data quality, and then convert loan_status_grouped to a string (object) to ensure proper classification handling in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5bdc6999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loan_status         object\n",
      "annual_income      float64\n",
      "fico_range_high    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# columns to drop na values from\n",
    "critical_columns = [\n",
    "    'loan_status',\n",
    "    'annual_income',\n",
    "    'fico_range_high'\n",
    "]\n",
    "\n",
    "# drop NaN values from columns\n",
    "accepted_loans = accepted_loans.dropna(subset=critical_columns).copy()\n",
    "\n",
    "# converting loan_status to string\n",
    "accepted_loans['loan_status'] = accepted_loans['loan_status'].astype(str)\n",
    "\n",
    "# Checking data types of columns\n",
    "print(accepted_loans[['loan_status', 'annual_income', 'fico_range_high']].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa69710",
   "metadata": {},
   "source": [
    "##### 2.2 Create `loan_status_grouped` for Risk Classification\n",
    "**Purpose:**  \n",
    "This step groups the raw `loan_status` values into simplified buckets: **Fully Paid (Success), Charged Off (Failure), Current/Pending (Uncertain), and Other/Exclude (Irrelevant)**. This is essential for risk analysis as it accurately defines the outcomes needed for the target variable (is_default) and for subsequent filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "753d555f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of the new grouped loan status:\n",
      "loan_status_grouped\n",
      "Fully Paid         0.476299\n",
      "Current/Pending    0.403673\n",
      "Charged Off        0.119151\n",
      "Other/Exclude      0.000878\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#  the complete mapping for the unique statuses  \n",
    "status_mapping = {\n",
    "    # Success\n",
    "    'Fully Paid': 'Fully Paid',\n",
    "    \n",
    "    # Failure/Default (CRITICAL for losses)\n",
    "    'Charged Off': 'Charged Off',\n",
    "    'Default': 'Charged Off', # 'Default' is a final failure state\n",
    "    'Does not meet the credit policy. Status:Charged Off': 'Charged Off', # Treat as a failure\n",
    "    \n",
    "    # Uncertain/Pending (CRITICAL for later filtering)\n",
    "    'Current': 'Current/Pending',\n",
    "    'In Grace Period': 'Current/Pending',\n",
    "    'Late (31-120 days)': 'Current/Pending',\n",
    "    'Late (16-30 days)': 'Current/Pending',\n",
    "    \n",
    "    # Other/Exclude (Irrelevant to core risk modeling)\n",
    "    'Does not meet the credit policy. Status:Fully Paid': 'Other/Exclude',\n",
    "}\n",
    " \n",
    "#  Apply the mapping, defaulting to 'Other/Exclude' if any new status appears \n",
    "def group_loan_status_accurate(status):\n",
    "    \"\"\"Maps status using the dictionary, or defaults to 'Other/Exclude'.\"\"\"\n",
    "    status_clean = status.strip()\n",
    "    return status_mapping.get(status_clean, 'Other/Exclude')\n",
    "\n",
    "accepted_loans['loan_status_grouped'] = accepted_loans['loan_status'].apply(group_loan_status_accurate)\n",
    "\n",
    "# Validate the distribution of the new column\n",
    "print(\"Distribution of the new grouped loan status:\")\n",
    "print(accepted_loans['loan_status_grouped'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787addd5",
   "metadata": {},
   "source": [
    "##### 2.3 Stratified Sampling for Accepted Loans\n",
    "**Purpose:**  \n",
    "To create a smaller, manageable sample of the data for faster computation and exploration while preserving the exact proportions of loan outcomes (Fully Paid, Charged Off, Current/Pending). We use stratified sampling based on the cleaned `loan_status_grouped` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0c1908aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepted Loans Sample: (226066, 103)\n"
     ]
    }
   ],
   "source": [
    "# Stratified sampling for accepted loans by loan_status\n",
    "accepted_sample, _ = train_test_split(\n",
    "    accepted_loans,\n",
    "    test_size=0.9, # keep 10% for sample \n",
    "    stratify=accepted_loans['loan_status_grouped'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f'Accepted Loans Sample: {accepted_sample.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5de746",
   "metadata": {},
   "source": [
    "##### 2.4 Create Binary Target (`is_default`) and Filter Out Uncertain Loans\n",
    "**Purpose:**  \n",
    "This step prepares the data for core risk analysis by filtering out all uncertain loans (Current/Pending) and keeping only definitive outcomes. We then create the binary target variable, is_default (1 = Charged Off, 0 = Fully Paid), which is the essential analytical flag used to accurately quantify default rates across different loan segments and calculate historical profitability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4cdf5ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_sample_filtered = accepted_sample[\n",
    "    accepted_sample['loan_status_grouped'].isin(['Fully Paid','Charged Off'])\n",
    "].copy()\n",
    "\n",
    "accepted_sample_filtered['is_default']= accepted_sample_filtered['loan_status_grouped'].apply(\n",
    "    lambda x: 1 if x == 'Charged Off' else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d0e744",
   "metadata": {},
   "source": [
    "#### 3. Core Feature Engineering and Validation\n",
    "**Purpose:**  \n",
    "To transform raw, continuous data (**like Annual Income, Fico Score and DTI**) into discrete, risk-quantifying features (**like Income Brackets, Fico Bins and DTI Quintiles**). This process creates variables that are both highly predictive for risk analysis and directly actionable for underwriting and portfolio management policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b0702c",
   "metadata": {},
   "source": [
    "##### 3.1 Create `annual_income` Brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e12dad15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of default rate by income brackets :\n",
      "\n",
      "income_brackets\n",
      " $50k - $100k     50.64%\n",
      "< $50k            28.70%\n",
      " $100k - $150k    14.42%\n",
      " > $150k           6.23%\n",
      "Name: proportion, dtype: object\n"
     ]
    }
   ],
   "source": [
    "accepted_sample_filtered.annual_income.max()\n",
    "income_labels = ['< $50k',' $50k - $100k',' $100k - $150k',' > $150k']\n",
    "income_bins = [0, 50_000, 100_000, 150_000, accepted_sample_filtered['annual_income'].max()+1]\n",
    "accepted_sample_filtered['income_brackets'] = pd.cut(\n",
    "    accepted_sample_filtered['annual_income'], \n",
    "    bins=income_bins, \n",
    "    labels=income_labels, \n",
    "    include_lowest=True,\n",
    "    right=False # Ensure 50k lands in the 50k-100k bin, not the <50k bin\n",
    ")\n",
    "\n",
    "print(\"Distribution of default rate by income brackets :\\n\")\n",
    "print(accepted_sample_filtered['income_brackets'].value_counts(normalize=True).map('{:.2%}'.format))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2c7d21",
   "metadata": {},
   "source": [
    "##### 3.2 Create `dti_quintile` Brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a50bccdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed Default Rate by DTI Quintile:\n",
      " \n",
      "dti_quintile\n",
      "Q5 (Highest DTI)    27.02%\n",
      "Q4                  22.05%\n",
      "Q3                  19.05%\n",
      "Q2                  16.64%\n",
      "Q1 (Lowest DTI)     15.29%\n",
      "Name: is_default, dtype: object\n"
     ]
    }
   ],
   "source": [
    "accepted_sample_filtered['dti_quintile'] = pd.qcut(\n",
    "    accepted_sample_filtered['debt_to_income_ratio'],\n",
    "    q=5, # Creates 5 bins of equal population size\n",
    "    labels=[\n",
    "        'Q1 (Lowest DTI)',  # 0\n",
    "        'Q2',               # 1\n",
    "        'Q3',               # 2\n",
    "        'Q4',               # 3\n",
    "        'Q5 (Highest DTI)'  # 4\n",
    "    ],\n",
    "    duplicates='drop'\n",
    ")\n",
    "# validate dti_quintile \n",
    "dti_risk_analysis = accepted_sample_filtered.groupby('dti_quintile', observed=True)['is_default'].mean()\n",
    "print('Observed Default Rate by DTI Quintile:\\n ')\n",
    "print(dti_risk_analysis.sort_values(ascending=False).map('{:.2%}'.format))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9782ed15",
   "metadata": {},
   "source": [
    "##### 3.3 Create `fico_score` Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f1dec936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top FICO bins created:\n",
      "\n",
      "fico_bin\n",
      "Good (670-739)          96065\n",
      "Subprime/Poor (<670)    24035\n",
      "Very Good (740-799)     12985\n",
      "Excellent (800+)         1526\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Calculate fico score - axis=1 (row-wise)\n",
    "accepted_sample_filtered['fico_score'] = accepted_sample_filtered[['fico_range_high','fico_range_low']].mean(axis=1)\n",
    "max_score = accepted_sample_filtered['fico_score'].max()\n",
    "fico_bins = [\n",
    "    0,\n",
    "    670, # boundary for poor/fair\n",
    "    740, # good/very good\n",
    "    800, # very good/excellent\n",
    "    max_score+1\n",
    "]\n",
    "fico_labels = [\n",
    "    'Subprime/Poor (<670)', \n",
    "    'Good (670-739)', \n",
    "    'Very Good (740-799)', \n",
    "    'Excellent (800+)'\n",
    "]\n",
    "accepted_sample_filtered['fico_bin']= pd.cut(\n",
    "    accepted_sample_filtered['fico_score'],\n",
    "    bins=fico_bins,\n",
    "    labels=fico_labels,\n",
    "    right=False,  #The bins are inclusive on the left\n",
    "    include_lowest=True\n",
    ")\n",
    "print(\"Top FICO bins created:\\n\")\n",
    "print(accepted_sample_filtered['fico_bin'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dd4f5999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed Default Rate by Fico Bins:\n",
      " \n",
      "fico_bin\n",
      "Subprime/Poor (<670)    26.24%\n",
      "Good (670-739)          20.01%\n",
      "Very Good (740-799)     10.03%\n",
      "Excellent (800+)         6.68%\n",
      "Name: is_default, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# validate fico bins\n",
    "fico_risk_analysis = accepted_sample_filtered.groupby('fico_bin', observed=True)['is_default'].mean()\n",
    "print('Observed Default Rate by Fico Bins:\\n ')\n",
    "print(fico_risk_analysis.sort_values(ascending=False).map('{:.2%}'.format))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4009b6fc",
   "metadata": {},
   "source": [
    "#### 4. Secondary Feature Engineering (Categorical)\n",
    "**Purpose:**  \n",
    "To clean and consolidate granular categorical data (e.g., loan_purpose, term) into a limited set of high-level, actionable categories (e.g., 'Debt Cons', 'Other', '36 Mo'). This reduces noise, ensures every segment is large enough for robust risk analysis, and provides clear, digestible insights for policy recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7e02f9",
   "metadata": {},
   "source": [
    "##### 4.1 Simplify `purpose` Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a378f058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed Default Rate by Loan Purpose Group:\n",
      "\n",
      "purpose_grouped\n",
      "debt_consolidation    21.21%\n",
      "Other                 20.99%\n",
      "home_improvement      18.00%\n",
      "credit_card           16.83%\n",
      "Name: is_default, dtype: object\n"
     ]
    }
   ],
   "source": [
    "top_purposes = ['debt_consolidation', 'credit_card', 'home_improvement']\n",
    "accepted_sample_filtered['purpose_grouped']=accepted_sample_filtered['purpose'].apply(lambda x: x if x in top_purposes else 'Other')\n",
    "\n",
    "# Validate Loan Purpose Groups\n",
    "purpose_risk_analysis = accepted_sample_filtered.groupby('purpose_grouped')['is_default'].mean()\n",
    "print('Observed Default Rate by Loan Purpose Group:\\n')\n",
    "print(purpose_risk_analysis.sort_values(ascending=False).map('{:.2%}'.format))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2342464",
   "metadata": {},
   "source": [
    "##### 4.2 Clean `loan_term`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "810c2f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed Default Rate by Loan Term:\n",
      "\n",
      "term_num\n",
      "36    16.07%\n",
      "60    32.38%\n",
      "Name: is_default, dtype: object\n"
     ]
    }
   ],
   "source": [
    "accepted_sample_filtered['term_num'] =  (\n",
    "    accepted_sample_filtered['term']\n",
    "    .str.replace(' months','', regex=False)\n",
    "    .str.strip()\n",
    "    .astype(int)\n",
    ")\n",
    "# Validate Loan term\n",
    "term_risk_analysis = accepted_sample_filtered.groupby('term_num')['is_default'].mean()\n",
    "print('Observed Default Rate by Loan Term:\\n')\n",
    "print(term_risk_analysis.map('{:.2%}'.format))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9e7662",
   "metadata": {},
   "source": [
    "##### 5. Final Cleanup and Export\n",
    "**Purpose:**  \n",
    "To finalize the engineered dataset by selecting only the essential analytical features (risk segments, financial terms, and the target variable). This step establishes a crucial data checkpoint by exporting the clean subset to a new file (`accepted_loans_sample.csv`). Exporting ensures that all intensive data cleaning and feature engineering steps are preserved, allowing subsequent analysis notebooks (EDA, Visualization) to load a fully prepared, concise, and ready-to-use dataset, avoiding redundant processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "87119eb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'debt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23292\\2243877311.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maccepted_sample_filtered\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\satka\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6295\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6296\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6297\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6298\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6299\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'debt'"
     ]
    }
   ],
   "source": [
    "accepted_sample_filtered.debt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_colunms = [\n",
    "    'is_default', # Target variable\n",
    "    # key loan terms\n",
    "    'amount_requested',\n",
    "    'funded_amount',\n",
    "    'interest_rate',\n",
    "    'installment',\n",
    "    'term_num',\n",
    "    # core credit risk segments\n",
    "    'fico_bin',\n",
    "    'dti_quintile',\n",
    "    'income_brackets',\n",
    "    'purpose_grouped',\n",
    "    # core credit risk scores\n",
    "    'fico_score',\n",
    "    'annual_income',\n",
    "    'debt_to_income_ratio',\n",
    "    # original risk grades\n",
    "    'grade',\n",
    "    'sub_grade',\n",
    "    # other\n",
    "    'total_bc_limit',\n",
    "    'total_il_high_credit_limit',\n",
    "    # date\n",
    "    'application_date'\n",
    "]\n",
    "\n",
    "# Save sampled and clean datasets to CSV\n",
    "final_loan_data = accepted_sample_filtered[final_colunms].copy()\n",
    "final_loan_data.to_csv('../data/sample_data/final_loan_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2855e490",
   "metadata": {},
   "source": [
    "##### 6. Summary and Next Steps\n",
    "##### Summary\n",
    "\n",
    "This notebook successfully completed all necessary **Feature Engineering (FE)** and data preparation steps on the stratified sample, resulting in a dataset ready for advanced risk analysis.\n",
    "\n",
    "1. Target Creation: The dataset was filtered to loans with definitive outcomes (Fully Paid vs. Charged Off) to create the binary target variable, is_default.\n",
    "2. Core Segment Engineering: We transformed the most predictive continuous variables into discrete, actionable risk segments:\n",
    "  - `fico_bin`: Standard industry tiers (e.g., Subprime, Good).\n",
    "  - `dti_quintile`: Five equal-sized population groups based on Debt-to-Income (DTI).\n",
    "  - `income_brackets`: Fixed tiers for Annual Income (< $50k, etc.).\n",
    "3. Secondary Feature Cleaning: Critical categorical variables were cleaned and validated:\n",
    "  - `purpose_grouped`: Consolidated rare loan_purpose categories into a single 'Other' bucket.\n",
    "  - `term_num`: Cleaned the loan term from a string to a numeric integer (36 or 60).\n",
    "4. Validation: Every engineered feature was validated by calculating the Observed Default Rate (ODR) per segment, confirming all expected risk trends.\n",
    "5. Checkpoint: The final clean, engineered sample was saved as final_loan_data.csv, serving as the input for the next phase.\n",
    "\n",
    "##### Next Steps\n",
    "\n",
    "The next notebook will focus entirely on Exploratory Data Analysis (EDA) and Visualization to graphically present the key risk findings and prepare the data for predictive modeling.\n",
    "\n",
    "**Planned Tasks:**\n",
    "1. Risk Trend Visualization: Plot the Observed Default Rate (ODR) for every engineered segment (`fico_bin`, `dti_quintile`, `income_brackets`) to visually demonstrate their predictive power.\n",
    "2. Univariate Analysis: Visualize the distributions of key features (`amount_requested`, `interest_rate`) to understand the borrower population.\n",
    "3. Bivariate Analysis: Investigate correlations and interactions between features (e.g., Interest Rate vs. Default Rate across different FICO Bins).\n",
    "4. Conclusion: Summarize the key findings that will directly inform the final predictive model design and business policy recommendations.\n",
    "The next phase will be documented in [`3_Bivariate_EDA.ipynb`](3_Bivariate_EDA.IPYNB)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
